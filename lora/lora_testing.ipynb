{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a4f39b-2dec-469d-8cd9-a9c81a54c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_dir = \"fine_tuned_lora_llama\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, model_dir)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"Fine-tuned model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df35da-695c-4fcd-8f9a-1ed919730784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Write a detailed job description for a Data Scientist role. He has to be an expert in data science and machine learning. He has to be able to understand and interpret data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the data. He has to be able to interpret the data and make intelligent decisions based on the\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate text using the fine-tuned model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Write a detailed job description for a Data Scientist role. He has to\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcbe0c-abc8-463b-ab1e-8e83efee6a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d0927-851e-43a7-9e8e-e39de209d47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline-venv",
   "language": "python",
   "name": "pipeline_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
